
\dates{May '19 - present} 
\location{Upton, New York}
\title{Computer Scientist}
\employer{Brookhaven National Laboratory}

{\bf Brookhaven National Laboratory $\>$$\>$$\>$$\>$ \textit{Computer Scientist} $\>$$\>$$\>$$\>$\textit{May 2019 - present}}
\begin{itemize}
    \item Working on locality-aware loop scheduling strategies, and generally user-defined loop schedules, in the context of MPI+OpenMP applications.
\item  Developing novel performance tuning techniques for MPI+OpenMP applications for emerging supercomputers, with a focus on new and challenging computational simulations and next-generation computer architectures.
\item Serving as Program Manager for DoE Exascale Computing Project's project SOLLVE and as Brookhaven National Laboratory's representative in the OpenMP Architecture Review Board.
\end{itemize}

\dates{Jun. 2018 - Apr. 2019}
\location{Champaign, Illinois}
\title{Software Developer}
\employer{Charmworks, Inc.} 

{\bf Charmworks, Inc. $\>$$\>$$\>$$\>$ \textit{Research Software Developer} $\>$$\>$$\>$$\>$ \textit{Jun. 2018 - Apr. 2019}}
%\begin{position}
\vspace{0.0in}
\begin{itemize}
\item Proposed examples of loop scheduling in OpenMP in examples section of OpenMP 5.1 Specification.
\item Submitted a ticket to be voted on by the OpenMP Language Committee to add an OpenMP User-defined Schedule to the OpenMP specification based on my OpenMPCon 2017 extended abstract, and presented the ticket at an OpenMP Face-to-Face meeting in January 2019.
\item Integrated a shared memory library for sophisticated loop scheduling strategies, including some based on my dissertation, into Charm++ version 6.9.
\item Worked on making User-defined Loop Scheduling portable across different parallel programming libraries, done with ORNL (Oscar Hernandez) and under the US DoE Exascale Computing Program.
\item Worked on an NSF startup SBIR proposal for low-overhead loop scheduling for desktop computers.
\item Assisted with slides for pitch and marketing of Charm++, and provided feedback for tutorials on Charm++.
%item Comparing performance of a loop scheduling strategy available in the integrated shared memory library with the performance of the corresponding loop scheduling strategy available in LLVM’s OpenMP library.
\end{itemize}
%\end{position}

{\bf University of Southern California $\>$$\>$$\>$$\>$ \textit{Computer Scientist} $\>$$\>$$\>$$\>$ \textit{Dec. 2016 - Jun 2018}}
\vspace*{-0.0in} 
\begin{itemize}
\item Worked in team to create a methodology to determine time needed to run an application program involving Fast Fourier Transformation and image reconstruction algorithms on an arbitrary number of GPUs of a cluster.
\item Translated an X-ray tomography code written in Matlab to C code and then parallelized the code to run on a supercomputer having nodes with GPUs.
\item Worked with the OpenMP Language Committee to begin discussion of a proposal to support User-defined Loop Schedules in OpenMP. 
\item Drafted modifications to the LLVM's OpenMP compiler and runtime system to support new OpenMP loop schedules. 
\item Worked with a postdoc from LLNL on a proposal to study performance optimization strategies that synergize a loop scheduling strategy with a load balancing strategy to improve performance of scientific applications.
\item Worked on ensuring that external network infrastructure, e.g., ESNet, to support transfer of X-ray imaging application code's input data files were adequate for an application code's efficient execution using the Globus Toolkit.
%\item \small Managing a git repository for a team working on
%performance optimizations of the application program.
%\item \small Doing optimizations for MPI+CUDA application code involving low-overhead loop scheduling and loop optimizations such as loop unrolling. 
%\item \small Working on transformations in LLVM. 
\end{itemize}

%TODO: adaptive VS hybrid VS ... 
{\bf Charmworks, Inc. $\>$$\>$$\>$$\>$ \textit{Software Developer} $\>$$\>$$\>$$\>$ \textit{Jan. 2016 - Nov. 2016}}
\vspace*{-0.0in}
\begin{itemize}
\item Implemented mixed static/dynamic loop scheduling strategies from my dissertation within Charm++'s thread scheduling library.
%TODO: consider adding 'including in cloud environments' the end of
%the sentence. 
%TODO: make paragraph 
\item Helped to improve portability of Charm++ to a variety of platforms. 
\item Assisted with business aspects of a high-tech startup.
\end{itemize}

{\bf University of Illinois $\>$$\>$$\>$$\>$ \textit{\textbf{Postdoctoral Associate}} $\>$$\>$$\>$$\>$ \textit{Jul. 2015 – Dec. 2015}}
\vspace*{-0.0in}
\begin{itemize} 
\item Helped to adapt an MPI plasma physics application code called PlasComCM to work on an NVIDIA GPU and run efficiently on an Intel Xeon Phi.
\item Worked on incorporating over-decomposition and locality-aware scheduling strategies into strategies from my dissertation.
\item Developed loop scheduling library that allows application programmers to use strategies from my dissertation in his or her application.
\end{itemize}

{\bf Lawrence Livermore National Laboratory$\>$$\>$$\>$$\>$ \textit{Lawrence Scholar} $\>$$\>$$\>$$\>$ \textit{Feb. 2012 – Jun. 2014}}
\vspace*{-0.0in}
\begin{itemize} 
\item Measured MPI communication delays of MPI communication functions in micro-benchmarks and application code run on supercomputers, and worked to find tools to measure dequeue overheads of OpenMP loop schedulers.
\item Created a software system for automated performance optimization and application programmer usability of low-overhead hybrid scheduling strategies.
\item Developed a ROSE-based custom compiler for automatically transforming MPI+OpenMP applications to use low-overhead scheduling techniques.
\item Assessed further opportunities for performance improvement of low-overhead schedulers, including improvement of spatial locality of low-overhead schedulers.
\end{itemize}

{\bf Lawrence Livermore National Laboratory$\>$$\>$$\>$$\>$\textit{Scholar}$\>$$\>$$\>$$\>$ \textit{Jun. 2011 - Sep. 2011}}
\vspace*{-0.0in}
\begin{itemize} 
\item Experimented with different OpenMP performance tuning parameters of an MPI+OpenMP application code to understand how to improve performance of applications run on LLNL's supercomputers.
\item Designed software architecture low-overhead loop scheduling library based on understanding software architecture of {\tt libgomp}.
\end{itemize} 

{\bf Lawrence Berkeley National Laboratory$\>$$\>$$\>$$\>$\textit{Scholar} $\>$$\>$$\>$$\>$\textit{Aug. 2010 - Sep. 2010}}
\begin{itemize}
\item Analyzed results for performance tests of Berkeley UPC collective functions run on NERSC supercomputers.
\item Worked on developing a methodology to compare performance of UPC collectives run with Berkeley UPC and GasNet with MPI collectives run with MPI's reference implementation, mpich2.
\end{itemize}

{\bf Lawrence Livermore National Laboratory$\>$$\>$$\>$$\>$\textit{Scholar}$\>$$\>$$\>$$\>$ \textit{May. 2010 - Aug. 2010}}
\begin{itemize}
\item Modified {\tt libgomp} runtime system in order to integrate low-overhead schedulers within it.
\item Developed a multi-stage low-overhead loop scheduler with each stage associated with a level in the memory hierarchy, allowing for MPI-shared memory extensions to be used with the low-overhead loop scheduling strategies.
\end{itemize}

{\bf Goldman-Sachs$\>\>\>\>$\textit{Summer Analyst}$\>\>\>\>$\textit{Jun. 2009 – Sep. 2009}}
\vspace*{-0.0in}
\begin{itemize}
\item Wrote code for testing trading system infrastructure functions under extreme market conditions.
\item Analyzed performance bottlenecks of system infrastructure functions.
\end{itemize}


%{\bf Proteus Technologies, LLC$\>\>\>\>$\textit{Software Developer}$\>\>\>\>$\textit{Aug. 2007 – Apr. 2008}}
%\vspace*{-0.0in}
%\begin{itemize} 
%\item Designed and implemented algorithms for cost optimization applications, using dynamic programming and discrete optimization heuristics.
%\item Was primarily responsible for developing, testing and documenting a service-oriented software application for health and status monitoring of large-scale parallel and distributed networked systems.
%\item Developed company standards for software development, i.e., System Requirements Specifications, Design Documentation.
%\end{itemize}